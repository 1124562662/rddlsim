{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Categorize(data_preprocess, N_category):\n",
    "    max1 = max(data_preprocess.max())\n",
    "    min1 = min(data_preprocess.min())\n",
    "    data_afterprocess = round((max1 - data_preprocess)/(max1-min1)*(N_category-1))*((max1-min1)/(N_category-1))+min1\n",
    "    return data_afterprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#import random\n",
    "path_data = 'ROOM_6/HVAC_Data.txt'\n",
    "path_label ='ROOM_6/HVAC_Label.txt'\n",
    "path_reward = 'ROOM_6/HVAC_Reward.txt'\n",
    "data_action_state = pd.read_table(path_data, sep = \",\")\n",
    "data_next_state_pre = pd.read_table(path_label, sep = \",\")\n",
    "data_reward = pd.read_table(path_reward, sep = \",\")\n",
    "# Separate Action, State, and Next Staet\n",
    "# We remove the state AIR_VOLUMN, which was created for visualization\n",
    "data_action_pre = data_action_state.filter(regex='action')\n",
    "data_state_pre = data_action_state.filter(regex='state')\n",
    "data_state_pre_dropped = data_state_pre.drop(data_state_pre.columns[6:12], axis = 1) \n",
    "data_next_state_pre_drop = data_next_state_pre.drop(data_next_state_pre.columns[6:12], axis = 1) \n",
    "\n",
    "N_room = 6; \n",
    "\n",
    "#We cannot have continuous data in action and state, so we need to divide them into N categories\n",
    "N_action = 20;\n",
    "# N_temp = 30;\n",
    "data_action = Categorize(data_action_pre, N_action)\n",
    "# data_state = Categorize(data_state_pre_dropped, N_temp)\n",
    "# Validate: data_action['action: AIR[$r2]'].unique()\n",
    "# Validate: data_state['states: TEMP[$r1]'].unique()\n",
    "\n",
    "data_state = round(data_state_pre_dropped,1)\n",
    "data_next_state = round(data_next_state_pre_drop,1)\n",
    "\n",
    "# Merge dataset: [action, state, new state, reward]\n",
    "data = pd.concat([data_action, data_state, data_next_state,data_reward], axis = 1)\n",
    "\n",
    "#Create training, validation, and testing set\n",
    "length = len(data)\n",
    "#random.shuffle(data)\n",
    "data_shuffle = data.reindex(np.random.permutation(data.index))\n",
    "data_train = data_shuffle[:int(0.8*length)]\n",
    "# data_validate = data_shuffle[int(0.5*length):int(0.8*length)]\n",
    "data_testing = data_shuffle[int(0.8*length):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The original state data in input is [1,6] array, we will transform it into [N_room, N_temp] hot-code format\n",
    "input_state = tf.placeholder(shape = [N_room,N_room], dtype = tf.float32)\n",
    "Weight = tf.Variable(tf.random_uniform([N_room, N_action], 0, 0.01))\n",
    "Q_out = tf.matmul(input_state, Weight) # All possible Q with each action in the current state\n",
    "predict = tf.argmax(Q_out, 1) # Find optimal action to achieve the max Q in each row\n",
    "\n",
    "Q_next = tf.placeholder(shape = [1, N_action], dtype = tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(Q_next-Q_out))\n",
    "trainer = trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Transition(Action, State):\n",
    "    # If the pair of Action and State is found in the training data, return the corresponding Next State and Rewards\n",
    "    # Otherwise, find the action that maximize the reward in that state in the training set, and return the \n",
    "    # correspondingNext State, adn Rewards\n",
    "    \n",
    "    # Return State_Next, Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "\n",
    "#Set up learning parameters\n",
    "num_episodes = 2000;\n",
    "y = 0.99 #Discount Factor\n",
    "\n",
    "\n",
    "# Create arraylist to record Rewards\n",
    "rList = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset the first observation in the new episodes\n",
    "        s = [10]*N_room # The first row of the data\n",
    "        j = 0 # j is the number of steps, and set the maximum as 99 steps\n",
    "        \n",
    "        # The Q-Network\n",
    "        while j < 99: \n",
    "            j+= 1\n",
    "            \n",
    "            # Use Weight to calcualte all the Q, and choose the optimal action and record all Q with potential Action\n",
    "            a,Q_all = sess.run([predict,Q_out],feed_dict={input_state:np.identity(N_room)*s}) \n",
    "            \n",
    "            #if np.random.rand(1) < e:\n",
    "            #   a = env.action_space.sample() # Exploration\n",
    "            \n",
    "            # Get the new state and reward from the environment\n",
    "            s1, r = transition(a, s)\n",
    "            \n",
    "            # Obtain the next Q by feeding the new state through the network\n",
    "            Q1 = sess.run(Q_out, feed_dict={input_state:np.identity(N_room)*s1})\n",
    "            \n",
    "            # Obtain Maximum of Q_new and set as our target\n",
    "            Q1_max = np.max(Q1)\n",
    "            target_Q = r + y*Q1_max #Bellmen Equation\n",
    "            # Train our network using target and predicted Q value\n",
    "            _,W1 = sess.run([updateModel, Weight],feed_dict={input_state:np.identity(N_room)*s,Q_next:target_Q})\n",
    "            r_all += r # Accumulate reward\n",
    "            s = s1 # Enter the next state\n",
    "        rList.append(r_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:IntroToTensorFlow]",
   "language": "python",
   "name": "conda-env-IntroToTensorFlow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
